{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0791de1b",
   "metadata": {
    "id": "0791de1b",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# 图像特征练习\n",
    "<details><summary>Image features exercise</summary>\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "We have seen that we can achieve reasonable performance on an image classification task by training a linear classifier on the pixels of the input image. In this exercise we will show that we can improve our classification performance by training linear classifiers not on raw pixels but on features that are computed from the raw pixels.\n",
    "\n",
    "All of your work for this exercise will be done in this notebook.\n",
    "</details>\n",
    "\n",
    "你需要完成并提交本练习（包括输出和任何支持代码）。更多细节请参见课程网站上的[作业页面](http://vision.stanford.edu/teaching/cs231n/assignments.html)。\n",
    "\n",
    "我们已经看到，通过在输入图像像素上训练线性分类器，可以在图像分类任务上取得合理的性能。在本练习中，我们将展示，通过在从原始像素计算出的特征上训练线性分类器，可以进一步提升分类性能。\n",
    "\n",
    "你所有的练习内容都将在本笔记本中完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e23d75",
   "metadata": {
    "id": "86e23d75",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # 设置默认绘图尺寸\n",
    "plt.rcParams['image.interpolation'] = 'nearest' # 设置图像插值方式\n",
    "plt.rcParams['image.cmap'] = 'gray' # 设置图像默认色彩映射\n",
    "\n",
    "# 用于自动重新加载外部模块\n",
    "# 参考 http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcba4a5",
   "metadata": {
    "id": "edcba4a5",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## 加载数据\n",
    "<details><summary>Load data</summary>\n",
    "Similar to previous exercises, we will load CIFAR-10 data from disk.\n",
    "</details>\n",
    "与之前的练习类似，我们将从磁盘加载 CIFAR-10 数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44caba8f",
   "metadata": {
    "id": "44caba8f",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "from cs231n.features import color_histogram_hsv, hog_feature\n",
    "\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    # 加载原始 CIFAR-10 数据\n",
    "    cifar10_dir = \"cs231n/datasets/cifar-10-batches-py\"\n",
    "\n",
    "    # 清理变量，防止多次加载数据导致内存问题\n",
    "    try:\n",
    "        del X_train, y_train\n",
    "        del X_test, y_test\n",
    "        print(\"Clear previously loaded data.\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # 子采样数据\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e4b3f6",
   "metadata": {
    "id": "37e4b3f6",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## 提取特征\n",
    "<details><summary>Extract Features</summary>\n",
    "For each image we will compute a Histogram of Oriented\n",
    "Gradients (HOG) as well as a color histogram using the hue channel in HSV\n",
    "color space. We form our final feature vector for each image by concatenating\n",
    "the HOG and color histogram feature vectors.\n",
    "\n",
    "Roughly speaking, HOG should capture the texture of the image while ignoring\n",
    "color information, and the color histogram represents the color of the input\n",
    "image while ignoring texture. As a result, we expect that using both together\n",
    "ought to work better than using either alone. Verifying this assumption would\n",
    "be a good thing to try for your own interest.\n",
    "\n",
    "The `hog_feature` and `color_histogram_hsv` functions both operate on a single\n",
    "image and return a feature vector for that image. The extract_features\n",
    "function takes a set of images and a list of feature functions and evaluates\n",
    "each feature function on each image, storing the results in a matrix where\n",
    "each column is the concatenation of all feature vectors for a single image.\n",
    "</details>\n",
    "对于每张图片，我们将计算方向梯度直方图（HOG）以及 HSV 色彩空间中色调通道的颜色直方图。我们通过将 HOG 和颜色直方图特征向量拼接，形成每张图片的最终特征向量。\n",
    "\n",
    "粗略来说，HOG 能捕捉图像的纹理信息而忽略颜色信息，颜色直方图则表示输入图像的颜色而忽略纹理。因此，结合两者应该比单独使用其中之一效果更好。你可以自行验证这个假设。\n",
    "\n",
    "`hog_feature` 和 `color_histogram_hsv` 函数都作用于单张图片，并返回该图片的特征向量。`extract_features` 函数接受一组图片和特征函数列表，对每张图片应用每个特征函数，并将所有特征向量拼接后存储在一个矩阵中，每一列对应一张图片的所有特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341d6c7",
   "metadata": {
    "id": "7341d6c7",
    "scrolled": true,
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "from cs231n.features import *\n",
    "\n",
    "# num_color_bins = 10 # 颜色直方图的 bin 数量\n",
    "num_color_bins = 25  # 颜色直方图的 bin 数量\n",
    "feature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n",
    "X_train_feats = extract_features(X_train, feature_fns, verbose=True)\n",
    "X_val_feats = extract_features(X_val, feature_fns)\n",
    "X_test_feats = extract_features(X_test, feature_fns)\n",
    "\n",
    "# 预处理：减去均值特征\n",
    "mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats -= mean_feat\n",
    "X_val_feats -= mean_feat\n",
    "X_test_feats -= mean_feat\n",
    "\n",
    "# 预处理：除以标准差。这样可以确保每个特征具有大致相同的尺度。\n",
    "std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats /= std_feat\n",
    "X_val_feats /= std_feat\n",
    "X_test_feats /= std_feat\n",
    "\n",
    "# 预处理：添加偏置维度\n",
    "X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n",
    "X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\n",
    "X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ded7b",
   "metadata": {
    "id": "684ded7b"
   },
   "source": [
    "## 在特征上训练 Softmax 分类器\n",
    "<details><summary>Train Softmax classifier on features</summary>\n",
    "Using the Softmax code developed earlier in the assignment, train Softmax classifiers on top of the features extracted above; this should achieve better results than training them directly on top of raw pixels.\n",
    "</details>\n",
    "使用之前作业中开发的 Softmax 代码，在上面提取的特征上训练 Softmax 分类器；这应该比直接在原始像素上训练效果更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abaf16c",
   "metadata": {
    "id": "9abaf16c",
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# 使用验证集调优学习率和正则化强度\n",
    "\n",
    "from cs231n.classifiers.linear_classifier import Softmax\n",
    "\n",
    "learning_rates = [1e-7, 1e-6]\n",
    "regularization_strengths = [5e5, 5e6]\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# 使用验证集设置学习率和正则化强度。                                                 #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# 这部分应与之前 Softmax 的验证过程一致；将最佳训练分类器保存在 best_softmax。          #\n",
    "# This should be identical to the validation that you did for the Softmax; save#\n",
    "# 如果你仔细调参，验证集准确率应能超过 0.42。                                        #\n",
    "# the best trained classifer in best_softmax. If you carefully tune the model, #\n",
    "#                                                                              #\n",
    "# you should be able to get accuracy of above 0.42 on the validation set.      #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# 输出结果。\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print(\"lr %e reg %e train accuracy: %f val accuracy: %f\" % (lr, reg, train_accuracy, val_accuracy))\n",
    "\n",
    "print(\"best validation accuracy achieved: %f\" % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54135a4",
   "metadata": {
    "id": "f54135a4",
    "test": "svm_test_accuracy"
   },
   "outputs": [],
   "source": [
    "# 在测试集上评估你训练的 Softmax：准确率应至少达到 0.42\n",
    "y_test_pred = best_softmax.predict(X_test_feats)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5i2oz_jdndHL",
   "metadata": {
    "id": "5i2oz_jdndHL"
   },
   "outputs": [],
   "source": [
    "# 保存最佳 softmax 模型\n",
    "best_softmax.save(\"best_softmax_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879145a",
   "metadata": {
    "id": "e879145a"
   },
   "outputs": [],
   "source": [
    "# 通过可视化错误分类的图片，可以帮助我们理解算法的工作方式。\n",
    "# 在这个可视化中，我们展示了被当前系统错误分类的图片。\n",
    "# 第一列展示了被系统标记为“plane”但真实标签不是“plane”的图片。\n",
    "\n",
    "examples_per_class = 8\n",
    "classes = [\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "for cls, cls_name in enumerate(classes):\n",
    "    idxs = np.where((y_test != cls) & (y_test_pred == cls))[0]\n",
    "    idxs = np.random.choice(idxs, examples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)\n",
    "        plt.imshow(X_test[idx].astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(cls_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bdb48",
   "metadata": {
    "id": "a70bdb48",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "### 内嵌问题 1：\n",
    "<details><summary>Inline question 1:</summary>\n",
    "Describe the misclassification results that you see. Do they make sense?\n",
    "</details>\n",
    "请描述你看到的错误分类结果。它们合理吗？\n",
    "\n",
    "$\\color{blue}{\\textit 你的答案：}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2d201",
   "metadata": {
    "id": "4fd2d201"
   },
   "source": [
    "## 在图像特征上训练神经网络\n",
    "<details><summary>Neural Network on image features</summary>\n",
    "Earlier in this assigment we saw that training a two-layer neural network on raw pixels achieved better classification performance than linear classifiers on raw pixels. In this notebook we have seen that linear classifiers on image features outperform linear classifiers on raw pixels.\n",
    "\n",
    "For completeness, we should also try training a neural network on image features. This approach should outperform all previous approaches: you should easily be able to achieve over 55% classification accuracy on the test set; our best model achieves about 60% classification accuracy.\n",
    "</details>\n",
    "在本作业的前面，我们看到在原始像素上训练两层神经网络比线性分类器效果更好。在本笔记本中，我们也看到在图像特征上训练线性分类器比在原始像素上效果更好。\n",
    "\n",
    "为了完整性，我们还应该尝试在图像特征上训练神经网络。这种方法应该优于之前所有方法：你应该可以轻松在测试集上达到 55% 以上的准确率，我们的最佳模型能达到约 60%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f690a1fa",
   "metadata": {
    "id": "f690a1fa",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 预处理：移除偏置维度\n",
    "# 请确保只运行一次本单元格\n",
    "print(X_train_feats.shape)\n",
    "X_train_feats = X_train_feats[:, :-1]\n",
    "X_val_feats = X_val_feats[:, :-1]\n",
    "X_test_feats = X_test_feats[:, :-1]\n",
    "\n",
    "print(X_train_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f914d6",
   "metadata": {
    "id": "30f914d6",
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "from cs231n.classifiers.fc_net import TwoLayerNet\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "input_dim = X_train_feats.shape[1]\n",
    "hidden_dim = 500\n",
    "num_classes = 10\n",
    "\n",
    "data = {\n",
    "    \"X_train\": X_train_feats,\n",
    "    \"y_train\": y_train,\n",
    "    \"X_val\": X_val_feats,\n",
    "    \"y_val\": y_val,\n",
    "    \"X_test\": X_test_feats,\n",
    "    \"y_test\": y_test,\n",
    "}\n",
    "\n",
    "net = TwoLayerNet(input_dim, hidden_dim, num_classes)\n",
    "best_net = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# 训练一个两层神经网络用于图像特征。你可以像之前一样交叉验证参数。                        #\n",
    "# Train a two-layer neural network on image features. You may want to          #\n",
    "# 将你的最佳模型保存在 best_net 变量中。                                            #\n",
    "# cross-validate various parameters as in previous sections. Store your best    #\n",
    "#                                                                               #\n",
    "# model in the best_net variable.                                               #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54573a5e",
   "metadata": {
    "id": "54573a5e",
    "test": "nn_test_accuracy"
   },
   "outputs": [],
   "source": [
    "# 在测试集上运行你的最佳神经网络分类器。你应该能达到 58% 以上的准确率。\n",
    "# 通过仔细调参，也有可能超过 60%。\n",
    "\n",
    "y_test_pred = np.argmax(best_net.loss(data[\"X_test\"]), axis=1)\n",
    "test_acc = (y_test_pred == data[\"y_test\"]).mean()\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JgC8wNQijcA9",
   "metadata": {
    "id": "JgC8wNQijcA9"
   },
   "outputs": [],
   "source": [
    "# 保存最佳模型\n",
    "best_net.save(\"best_two_layer_net_features.npy\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs231n-25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
