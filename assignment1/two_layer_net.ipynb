{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e370a238",
   "metadata": {
    "id": "e370a238",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# 全连接神经网络\n",
    "在本练习中，我们将采用模块化方法实现全连接网络。对于每一层，我们都将实现一个 `forward` 和一个 `backward` 函数。`forward` 函数接收输入、权重和其他参数，并返回输出和一个用于反向传播的 `cache` 对象，如下所示：\n",
    "\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# Fully-Connected Neural Nets\n",
    "In this exercise we will implement fully-connected networks using a modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf2688",
   "metadata": {
    "id": "0acf2688",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 常规设置\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # 设置默认图像大小\n",
    "plt.rcParams['image.interpolation'] = 'nearest' # 设置图像插值方式\n",
    "plt.rcParams['image.cmap'] = 'gray' # 设置默认色图\n",
    "\n",
    "# 自动重载外部模块\n",
    "# 参考 http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" 返回相对误差 \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909acc7b",
   "metadata": {
    "id": "909acc7b",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 加载（预处理后的）CIFAR10数据。\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "  print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7851b32",
   "metadata": {
    "id": "e7851b32"
   },
   "source": [
    "# 仿射层：前向\n",
    "打开文件 `cs231n/layers.py` 并实现 `affine_forward` 函数。\n",
    "\n",
    "完成后可以运行以下代码测试你的实现：\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# Affine layer: forward\n",
    "Open the file `cs231n/layers.py` and implement the `affine_forward` function.\n",
    "\n",
    "Once you are done you can test your implementaion by running the following:\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408a08c",
   "metadata": {
    "id": "9408a08c"
   },
   "outputs": [],
   "source": [
    "# 测试 affine_forward 函数\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# 与我们的输出进行比较。误差应在 e-9 或更小。\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdfab60",
   "metadata": {
    "id": "bfdfab60"
   },
   "source": [
    "# 仿射层：反向\n",
    "现在实现 `affine_backward` 函数，并使用数值梯度检查测试你的实现。\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# Affine layer: backward\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebdc09",
   "metadata": {
    "id": "42ebdc09"
   },
   "outputs": [],
   "source": [
    "# 测试 affine_backward 函数\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# 误差应在 e-10 或更小\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b019e3",
   "metadata": {
    "id": "00b019e3"
   },
   "source": [
    "# ReLU 激活：前向\n",
    "在 `relu_forward` 函数中实现 ReLU 激活函数的前向传播，并使用以下代码测试你的实现：\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# ReLU activation: forward\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007bbf68",
   "metadata": {
    "id": "007bbf68"
   },
   "outputs": [],
   "source": [
    "# 测试 relu_forward 函数\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# 与我们的输出进行比较。误差应在 e-8 量级\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8a267",
   "metadata": {
    "id": "3ac8a267"
   },
   "source": [
    "# ReLU 激活：反向\n",
    "现在在 `relu_backward` 函数中实现 ReLU 激活函数的反向传播，并使用数值梯度检查测试你的实现：\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# ReLU activation: backward\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a1bb2",
   "metadata": {
    "id": "304a1bb2"
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# 误差应在 e-12 量级\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b23f3",
   "metadata": {
    "id": "e17b23f3",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## 题内问题 1:\n",
    "\n",
    "我们只要求你实现了 ReLU，但在神经网络中可以使用许多不同的激活函数，每种都有其优缺点。特别是，激活函数常见的问题是反向传播时梯度为零（或接近零）。以下哪些激活函数会出现这个问题？如果你考虑一维情况，什么类型的输入会导致这种行为？\n",
    "1. Sigmoid\n",
    "2. ReLU\n",
    "3. Leaky ReLU\n",
    "\n",
    "$\\color{blue}{\\textit 你的答案:}$ *请填写*\n",
    "\n",
    "<details><summary>英文原文</summary>\n",
    "## Inline Question 1:\n",
    "\n",
    "We've only asked you to implement ReLU, but there are a number of different activation functions that one could use in neural networks, each with its pros and cons. In particular, an issue commonly seen with activation functions is getting zero (or close to zero) gradient flow during backpropagation. Which of the following activation functions have this problem? If you consider these functions in the one dimensional case, what types of input would lead to this behaviour?\n",
    "1. Sigmoid\n",
    "2. ReLU\n",
    "3. Leaky ReLU\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *Fill this in*\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9daf016",
   "metadata": {
    "id": "e9daf016"
   },
   "source": [
    "# “三明治”层\n",
    "在神经网络中有一些常见的层组合模式。例如，仿射层后常常跟着 ReLU 非线性。为了方便，我们在 `cs231n/layer_utils.py` 文件中定义了几个便捷层。\n",
    "\n",
    "现在请查看 `affine_relu_forward` 和 `affine_relu_backward` 函数，并运行以下代码对反向传播进行数值梯度检查：\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `cs231n/layer_utils.py`.\n",
    "\n",
    "For now take a look at the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass:\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30088927",
   "metadata": {
    "id": "30088927"
   },
   "outputs": [],
   "source": [
    "from cs231n.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# 相对误差应在 e-10 或更小\n",
    "print('Testing affine_relu_forward and affine_relu_backward:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dd603b",
   "metadata": {
    "id": "37dd603b"
   },
   "source": [
    "# 损失层：Softmax\n",
    "现在在 `cs231n/layers.py` 文件中实现 softmax 的损失和梯度函数 `softmax_loss`。这些实现应与 `cs231n/classifiers/softmax.py` 中类似。其他损失函数（如 `svm_loss`）也可以模块化实现，但本次作业不要求。\n",
    "\n",
    "你可以通过运行以下代码确保实现正确：\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# Loss layers: Softmax\n",
    "Now implement the loss and gradient for softmax in the `softmax_loss` function in `cs231n/layers.py`. These should be similar to what you implemented in `cs231n/classifiers/softmax.py`. Other loss functions (e.g. `svm_loss`) can also be implemented in a modular way, however, it is not required for this assignment.\n",
    "\n",
    "You can make sure that the implementations are correct by running the following:\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5269d",
   "metadata": {
    "id": "22d5269d"
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# 测试 softmax_loss 函数。Loss 应接近 2.3，dx 误差应在 e-8 量级\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898b95a",
   "metadata": {
    "id": "5898b95a"
   },
   "source": [
    "# 两层网络\n",
    "打开文件 `cs231n/classifiers/fc_net.py`，完成 `TwoLayerNet` 类的实现。请仔细阅读并理解其 API。你可以运行下面的代码测试你的实现。\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# Two-layer network\n",
    "Open the file `cs231n/classifiers/fc_net.py` and complete the implementation of the `TwoLayerNet` class. Read through it to make sure you understand the API. You can run the cell below to test your implementation.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83db9b",
   "metadata": {
    "id": "0a83db9b"
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "# 误差应在 e-7 或更小\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf7b47",
   "metadata": {
    "id": "0adf7b47"
   },
   "source": [
    "# 求解器\n",
    "打开文件 `cs231n/solver.py`，熟悉其 API。之后，使用 `Solver` 实例训练一个 `TwoLayerNet`，使其在验证集上达到约 36% 的准确率。\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# Solver\n",
    "Open the file `cs231n/solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves about `36%` accuracy on the validation set.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13472b96",
   "metadata": {
    "id": "13472b96"
   },
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: 使用 Solver 实例训练一个 TwoLayerNet，使其在验证集上达到约 36% 的准确率。 #\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves about 36% #\n",
    "# accuracy on the validation set.                                            #\n",
    "##############################################################################\n",
    "\n",
    "##############################################################################\n",
    "#                             代码结束 END OF YOUR CODE                      #\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdfc7f4",
   "metadata": {
    "id": "ffdfc7f4"
   },
   "source": [
    "# 调试训练过程\n",
    "使用上述默认参数，你应该能在验证集上获得约 0.36 的准确率。这并不是很高。\n",
    "\n",
    "一种获得问题洞察的方法是，在优化过程中绘制损失函数以及训练集和验证集上的准确率。\n",
    "\n",
    "另一种方法是可视化网络第一层学习到的权重。在大多数针对视觉数据训练的神经网络中，第一层权重在可视化时通常会显示出一些明显的结构。\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# Debug the training\n",
    "With the default parameters we provided above, you should get a validation accuracy of about 0.36 on the validation set. This isn't very good.\n",
    "\n",
    "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n",
    "\n",
    "Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3649ea",
   "metadata": {
    "id": "1a3649ea"
   },
   "outputs": [],
   "source": [
    "# 运行此单元以可视化训练损失和训练/验证准确率\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6052910",
   "metadata": {
    "id": "a6052910"
   },
   "outputs": [],
   "source": [
    "from cs231n.vis_utils import visualize_grid\n",
    "\n",
    "# 可视化网络权重\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(3, 32, 32, -1).transpose(3, 1, 2, 0)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68386e68",
   "metadata": {
    "id": "68386e68"
   },
   "source": [
    "# 调整超参数\n",
    "\n",
    "**哪里出了问题？** 观察上面的可视化，我们发现损失基本线性下降，这表明学习率可能太低。此外，训练和验证准确率之间没有差距，说明我们使用的模型容量较低，应该增加模型规模。另一方面，如果模型过大，我们会看到训练和验证准确率之间有很大差距，这表明过拟合。\n",
    "\n",
    "**调参**。调整超参数并培养对其影响最终性能的直觉是使用神经网络的重要部分，因此我们希望你多加练习。下面，你应该尝试不同的超参数值，包括隐藏层大小、学习率、训练轮数和正则化强度。你也可以考虑调整学习率衰减，但使用默认值也能获得不错的效果。\n",
    "\n",
    "**预期结果**。你的目标是在验证集上获得超过 48% 的分类准确率。我们最好的网络在验证集上能达到 52% 以上。\n",
    "\n",
    "**实验**：本练习的目标是用全连接神经网络在 CIFAR-10 上获得尽可能好的结果（52% 可作为参考）。你可以自由实现自己的技巧（如 PCA 降维、添加 dropout、给 solver 增加新特性等）。\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# Tune your hyperparameters\n",
    "\n",
    "**What's wrong?**. Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.\n",
    "\n",
    "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.\n",
    "\n",
    "**Approximate results**. You should be aim to achieve a classification accuracy of greater than 48% on the validation set. Our best network gets over 52% on the validation set.\n",
    "\n",
    "**Experiment**: You goal in this exercise is to get as good of a result on CIFAR-10 as you can (52% could serve as a reference), with a fully-connected Neural Network. Feel free implement your own techniques (e.g. PCA to reduce dimensionality, or adding dropout, or adding features to the solver, etc.).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d308eb5",
   "metadata": {
    "id": "3d308eb5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "# TODO: 使用验证集调参，并将你训练的最佳模型存储在 best_model。                      #\n",
    "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
    "# model in best_model.                                                          #\n",
    "#                                                                               #\n",
    "# 为了调试你的网络，可以使用类似上面可视化的方式；这些可视化会与调参前的网络有显著的定性差异。 #\n",
    "# To help debug your network, it may help to use visualizations similar to the  #\n",
    "# ones we used above; these visualizations will have significant qualitative    #\n",
    "# differences from the ones we saw above for the poorly tuned network.          #\n",
    "#                                                                               #\n",
    "# 手动调参很有趣，但你可能会发现写代码自动遍历超参数组合会更有效，就像我们在前面的练习中做的那样。 #\n",
    "# Tweaking hyperparameters by hand can be fun, but you might find it useful to  #\n",
    "# write code to sweep through possible combinations of hyperparameters          #\n",
    "# automatically like we did on thexs previous exercises.                          #\n",
    "#################################################################################\n",
    "\n",
    "################################################################################\n",
    "#                              代码结束 END OF YOUR CODE                        #\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c70f18f",
   "metadata": {
    "id": "6c70f18f"
   },
   "source": [
    "# 测试你的模型！\n",
    "在验证集和测试集上运行你的最佳模型。你应该在验证集和测试集上都获得超过 48% 的准确率。\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# Test your model!\n",
    "Run your best model on the validation and test sets. You should achieve above 48% accuracy on the validation set and the test set.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e7589",
   "metadata": {
    "id": "666e7589",
    "test": "val_accuracy"
   },
   "outputs": [],
   "source": [
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d74b7b6",
   "metadata": {
    "id": "0d74b7b6",
    "test": "test_accuracy"
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JlZg7TkKfesG",
   "metadata": {
    "id": "JlZg7TkKfesG"
   },
   "outputs": [],
   "source": [
    "# 保存最佳模型\n",
    "best_model.save(\"best_two_layer_net.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e2ae9",
   "metadata": {
    "id": "ab8e2ae9"
   },
   "source": [
    "## 题内问题 2:\n",
    "\n",
    "现在你已经训练了一个神经网络分类器，你可能会发现测试准确率远低于训练准确率。我们可以通过哪些方式减少这种差距？请选择所有适用项。\n",
    "\n",
    "1. 用更大的数据集进行训练。\n",
    "2. 增加隐藏单元数量。\n",
    "3. 增加正则化强度。\n",
    "4. 以上都不是。\n",
    "\n",
    "$\\color{blue}{\\textit 你的答案:}$\n",
    "\n",
    "$\\color{blue}{\\textit 你的解释:}$\n",
    "\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "## Inline Question 2:\n",
    "\n",
    "Now that you have trained a Neural Network classifier, you may find that your testing accuracy is much lower than the training accuracy. In what ways can we decrease this gap? Select all that apply.\n",
    "\n",
    "1. Train on a larger dataset.\n",
    "2. Add more hidden units.\n",
    "3. Increase the regularization strength.\n",
    "4. None of the above.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
