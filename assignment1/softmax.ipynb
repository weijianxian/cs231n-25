{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08abd1b",
   "metadata": {
    "id": "f08abd1b",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax 分类器练习\n",
    "\n",
    "完成并提交此工作表（包括其输出和任何在工作表外的支持代码）作为你的作业提交。更多细节请参见课程网站上的[作业页面](http://vision.stanford.edu/teaching/cs231n/assignments.html)。\n",
    "\n",
    "在本练习中你将：\n",
    "\n",
    "- 实现 Softmax 分类器的全向量化**损失函数**。\n",
    "- 实现其**解析梯度**的全向量化表达式。\n",
    "- 使用数值梯度**检查你的实现**。\n",
    "- 使用验证集**调优学习率和正则化强度**。\n",
    "- 用**SGD**优化损失函数。\n",
    "- **可视化**最终学习到的权重。\n",
    "\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "# Softmax Classifier exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "In this exercise you will:\n",
    "    \n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier.\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** using numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defe387",
   "metadata": {
    "id": "3defe387",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 运行本笔记本的一些设置代码。\n",
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 这是一些魔法命令，使 matplotlib 图像在笔记本内联显示\n",
    "# 而不是在新窗口中显示。\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # 设置图像默认大小\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# 还有一些魔法命令，使笔记本能自动重新加载外部 python 模块；\n",
    "# 参考 http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8603125",
   "metadata": {
    "id": "b8603125",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## CIFAR-10 数据加载与预处理\n",
    "\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "## CIFAR-10 Data Loading and Preprocessing\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf85b0",
   "metadata": {
    "id": "23bf85b0",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 加载原始 CIFAR-10 数据。\n",
    "cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "\n",
    "# 清理变量以防止多次加载数据（可能导致内存问题）\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# 做一个健壮性检查，打印训练和测试数据的大小。\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ea325",
   "metadata": {
    "id": "471ea325",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 可视化数据集中的一些样本。\n",
    "# 我们展示每个类别的部分训练图像样本。\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1f984",
   "metadata": {
    "id": "9bd1f984",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 将数据分为训练集、验证集和测试集。此外我们还会\n",
    "# 创建一个小的开发集作为训练集的子集；\n",
    "# 用于开发阶段以加快代码运行速度。\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "# 验证集将是原始训练集中的 num_validation 个样本。\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# 训练集将是原始训练集中的前 num_train 个样本。\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# 我们还会创建一个开发集，是训练集的一个小子集。\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# 测试集是原始测试集的前 num_test 个样本。\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd02f9",
   "metadata": {
    "id": "9bdd02f9",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 预处理：将图像数据重塑为行向量\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "# 做一个健壮性检查，打印数据的形状\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066fffc7",
   "metadata": {
    "id": "066fffc7",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "# 预处理：减去均值图像\n",
    "# 第一步：根据训练数据计算均值图像\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "print(mean_image[:10]) # 打印部分元素\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # 可视化均值图像\n",
    "plt.show()\n",
    "\n",
    "# 第二步：从训练和测试数据中减去均值图像\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n",
    "# 第三步：添加偏置维度（全为 1），这样分类器只需优化一个权重矩阵 W。\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c22f8",
   "metadata": {
    "id": "605c22f8"
   },
   "source": [
    "## Softmax 分类器\n",
    "\n",
    "本节的代码都写在 `cs231n/classifiers/softmax.py` 文件中。\n",
    "\n",
    "如你所见，我们已经预填了 `softmax_loss_naive` 函数，它用 for 循环计算 softmax 损失函数。\n",
    "\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n",
    "\n",
    "As you can see, we have prefilled the function `softmax_loss_naive` which uses for loops to evaluate the softmax loss function.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56073f14",
   "metadata": {
    "id": "56073f14"
   },
   "outputs": [],
   "source": [
    "# 评估我们为你提供的朴素损失实现：\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# 生成一个随机的 Softmax 分类器权重矩阵，数值较小\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "print('loss: %f' % (loss, ))\n",
    "\n",
    "# 粗略健壮性检查，我们的损失应该接近 -log(0.1)。\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NT_PDkFhZlMy",
   "metadata": {
    "id": "NT_PDkFhZlMy"
   },
   "source": [
    "**内嵌问题 1**\n",
    "\n",
    "为什么我们期望损失接近 -log(0.1)？请简要解释。\n",
    "\n",
    "$\\color{blue}{\\textit 你的答案:}$ *请填写*\n",
    "\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *Fill this in*\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad4937",
   "metadata": {
    "id": "7dad4937"
   },
   "source": [
    "上面函数返回的 `grad` 目前全为零。请推导并实现 softmax 损失函数的梯度，并在 `softmax_loss_naive` 函数内实现。你可以将新代码插入到现有函数中。\n",
    "\n",
    "为了检查你是否正确实现了梯度，可以用数值方法估算损失函数的梯度，并与你计算的梯度进行比较。我们已为你提供了相关代码：\n",
    "\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "The `grad` returned from the function above is right now all zero. Derive and implement the gradient for the softmax loss function and implement it inline inside the function `softmax_loss_naive`. You will find it helpful to interleave your new code inside the existing function.\n",
    "\n",
    "To check that you have correctly implemented the gradient, you can numerically estimate the gradient of the loss function and compare the numeric estimate to the gradient that you computed. We have provided code that does this for you:\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d5ad22",
   "metadata": {
    "id": "f7d5ad22"
   },
   "outputs": [],
   "source": [
    "# 实现梯度后，重新计算并用我们提供的函数做梯度检查\n",
    "\n",
    "# 在 W 处计算损失和梯度。\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# 在几个随机选定的维度上数值计算梯度，\n",
    "# 并与你解析计算的梯度比较。所有维度上的数值应该几乎完全一致。\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)\n",
    "\n",
    "# 再次做梯度检查，打开正则化项\n",
    "# 你没有忘记正则化梯度吧？\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fc684",
   "metadata": {
    "id": "665fc684",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**内嵌问题 2**\n",
    "\n",
    "虽然 gradcheck 对 softmax 损失很可靠，但对于 SVM 损失，偶尔某个维度的 gradcheck 结果可能不完全匹配。造成这种差异的原因可能是什么？是否需要担心？请举一个一维情况下 SVM 损失梯度检查可能失败的简单例子？改变 margin 会如何影响这种情况发生的频率？\n",
    "\n",
    "注意，对于样本 $(x_i, y_i)$，SVM 损失定义为：$$L_i = \\sum_{j\\ne y_i}\\max(0, s_j - s_{y_i} + \\Delta)$$ 其中 $j$ 遍历所有非正确类别 $y_i$，$s_j$ 表示第 $j$ 类的分类器得分。$\\Delta$ 是一个标量间隔。更多信息见[这里](https://cs231n.github.io/linear-classify/)的“多类支持向量机损失”。\n",
    "\n",
    "*提示：SVM 损失函数严格来说并不可微分。*\n",
    "\n",
    "$\\color{blue}{\\textit 你的答案:}$ *请填写*\n",
    "\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "**Inline Question 2**\n",
    "\n",
    "Although gradcheck is reliable softmax loss, it is possible that for SVM loss, once in a while, a dimension in the gradcheck will not match exactly. What could such a discrepancy be caused by? Is it a reason for concern? What is a simple example in one dimension where a svm loss gradient check could fail? How would change the margin affect of the frequency of this happening?\n",
    "\n",
    "Note that SVM loss for a sample $(x_i, y_i)$ is defined as: $$L_i = \\sum_{j\\ne y_i}\\max(0, s_j - s_{y_i} + \\Delta)$$ where $j$ iterates over all classes except the correct class $y_i$ and $s_j$ denotes the classifier score for $j^{th}$ class. $\\Delta$ is a scalar margin. For more information, refer to 'Multiclass Support Vector Machine loss' on [this](https://cs231n.github.io/linear-classify/) page.\n",
    "\n",
    "*Hint: the SVM loss function is not strictly speaking differentiable.*\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *fill this in.*  \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ca148",
   "metadata": {
    "id": "310ca148",
    "scrolled": true,
    "test": "vectorized_time_1"
   },
   "outputs": [],
   "source": [
    "# 接下来实现 softmax_loss_vectorized 函数；目前只需计算损失，\n",
    "# 梯度稍后实现。\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('Naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, _ = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# 损失应该一致，但你的向量化实现应该快得多。\n",
    "print('difference: %f' % (loss_naive - loss_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d3643",
   "metadata": {
    "id": "ef5d3643",
    "test": "vectorized_time_2"
   },
   "outputs": [],
   "source": [
    "# 完成 softmax_loss_vectorized 的实现，并用向量化方式计算损失函数的梯度。\n",
    "\n",
    "# 朴素实现和向量化实现应该一致，但\n",
    "# 向量化版本仍然快得多。\n",
    "tic = time.time()\n",
    "_, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('Vectorized loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "# 损失是一个数值，所以比较两种实现的值很容易。梯度是一个矩阵，\n",
    "# 所以我们用 Frobenius 范数比较它们。\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('difference: %f' % difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635eef89",
   "metadata": {
    "id": "635eef89"
   },
   "source": [
    "### 随机梯度下降（SGD）\n",
    "\n",
    "现在我们有了向量化且高效的损失和梯度表达式，并且梯度与数值梯度一致。因此我们可以用 SGD 最小化损失。本部分代码写在 `cs231n/classifiers/linear_classifier.py` 文件中。\n",
    "\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "We now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss. Your code for this part will be written inside `cs231n/classifiers/linear_classifier.py`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a13952",
   "metadata": {
    "id": "a5a13952",
    "test": "sgd"
   },
   "outputs": [],
   "source": [
    "# 在 linear_classifier.py 文件中实现 SGD，\n",
    "# LinearClassifier.train() 函数，然后运行下面的代码。\n",
    "from cs231n.classifiers import Softmax\n",
    "softmax = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = softmax.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,\n",
    "                      num_iters=1500, verbose=True)\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad0c45",
   "metadata": {
    "id": "abad0c45"
   },
   "outputs": [],
   "source": [
    "# 一个有用的调试策略是绘制损失随迭代次数的变化曲线：\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54af4f",
   "metadata": {
    "id": "2e54af4f",
    "test": "validate"
   },
   "outputs": [],
   "source": [
    "# 编写 LinearClassifier.predict 函数，并在训练集和验证集上评估性能\n",
    "# 你应该能在验证集上得到约 0.34 的准确率（> 0.33）。\n",
    "y_train_pred = softmax.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = softmax.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O4ac8brXSxeo",
   "metadata": {
    "id": "O4ac8brXSxeo"
   },
   "outputs": [],
   "source": [
    "# 保存训练好的模型用于自动评分。\n",
    "softmax.save(\"softmax.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2da33",
   "metadata": {
    "id": "1ce2da33",
    "tags": [
     "code"
    ],
    "test": "tuning"
   },
   "outputs": [],
   "source": [
    "# 用验证集调优超参数（正则化强度和学习率）。你应该尝试不同范围的学习率和正则化强度；\n",
    "# 如果你仔细调参，应该能在验证集上得到约 0.365 的准确率（> 0.36）。\n",
    "\n",
    "# 注意：在超参数搜索过程中可能会看到运行时/溢出警告。\n",
    "# 这可能是极端值导致的，不是 bug。\n",
    "\n",
    "# results 是一个字典，键为 (learning_rate, regularization_strength) 元组，\n",
    "# 值为 (training_accuracy, validation_accuracy) 元组。准确率即正确分类样本的比例。\n",
    "results = {}\n",
    "best_val = -1   # 目前见过的最高验证准确率。\n",
    "best_softmax = None # 达到最高验证准确率的 Softmax 对象。\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# 编写代码，通过在验证集上调参选择最佳超参数。对于每组超参数，训练一个 Softmax，         #\n",
    "# 在训练集和验证集上计算准确率，并存入 results 字典。此外，将最佳验证准确率存入 best_val， #\n",
    "# 达到该准确率的 Softmax 对象存入 best_softmax。                                 #\n",
    "#                                                                              #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train a Softmax on the.        #\n",
    "# training set, compute its accuracy on the training and validation sets, and  #\n",
    "# store these numbers in the results dictionary. In addition, store the best   #\n",
    "# validation accuracy in best_val and the Softmax object that achieves this.   #\n",
    "# accuracy in best_softmax.                                                    #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your         #\n",
    "# validation code so that the classifiers don't take much time to train; once  #\n",
    "# you are confident that your validation code works, you should rerun the      #\n",
    "# code with a larger value for num_iters.                                      #\n",
    "################################################################################\n",
    "\n",
    "# 仅供参考。你可以根据需要修改这些超参数\n",
    "learning_rates = [1e-7, 1e-6]\n",
    "regularization_strengths = [2.5e4, 1e4]\n",
    "\n",
    "\n",
    "\n",
    "# 打印结果。\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "\n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b520723",
   "metadata": {
    "id": "1b520723",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "# 可视化交叉验证结果\n",
    "import math\n",
    "import pdb\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# 绘制训练准确率\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.tight_layout(pad=3)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 training accuracy')\n",
    "\n",
    "# 绘制验证准确率\n",
    "colors = [results[x][1] for x in results] # 默认 marker 大小为 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29768e",
   "metadata": {
    "id": "7b29768e",
    "test": "test"
   },
   "outputs": [],
   "source": [
    "# 在测试集上评估最佳 softmax\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('Softmax classifier on raw pixels final test set accuracy: %f' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JU4xmsWpUIAd",
   "metadata": {
    "id": "JU4xmsWpUIAd"
   },
   "outputs": [],
   "source": [
    "# 保存最佳 softmax 模型\n",
    "best_softmax.save(\"best_softmax.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e122c35",
   "metadata": {
    "id": "9e122c35",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "# 可视化每个类别学习到的权重。\n",
    "# 根据你的学习率和正则化强度选择，这些权重可能\n",
    "# 好看也可能不好看。\n",
    "w = best_softmax.W[:-1,:] # 去掉偏置\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # 将权重缩放到 0 到 255 之间\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b827ad",
   "metadata": {
    "id": "44b827ad",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**内嵌问题 3**\n",
    "\n",
    "描述你可视化的 Softmax 分类器权重是什么样的，并简要解释为什么会这样。\n",
    "\n",
    "$\\color{blue}{\\textit 你的答案:}$ *请填写*\n",
    "\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "**Inline question 3**\n",
    "\n",
    "Describe what your visualized Softmax classifier weights look like, and offer a brief explanation for why they look the way they do.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *fill this in*  \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DvBKlibgaiy9",
   "metadata": {
    "id": "DvBKlibgaiy9"
   },
   "source": [
    "**内嵌问题 4** - *判断对错*\n",
    "\n",
    "假设整体训练损失定义为所有训练样本的单点损失之和。可能添加一个新样本到训练集会改变 softmax 损失，但 SVM 损失保持不变。\n",
    "\n",
    "$\\color{blue}{\\textit 你的答案:}$\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit 你的解释:}$\n",
    "\n",
    "<details><summary>英文原文</summary>\n",
    "\n",
    "**Inline Question 4** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would change the softmax loss, but leave the SVM loss unchanged.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_gLYWFm7akSI",
   "metadata": {
    "id": "_gLYWFm7akSI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
