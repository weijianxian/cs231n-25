{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hmW4qyEu-pR"
   },
   "source": [
    "# 多层全连接网络\n",
    "在本练习中，你将实现一个具有任意数量隐藏层的全连接网络。\n",
    "<details><summary>原文</summary>\n",
    "# Multi-Layer Fully Connected Network\n",
    "In this exercise, you will implement a fully connected network with an arbitrary number of hidden layers.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "audT-ccNu-pT"
   },
   "source": [
    "阅读 `cs231n/classifiers/fc_net.py` 文件中的 `FullyConnectedNet` 类。\n",
    "\n",
    "实现网络的初始化、前向传播和反向传播。在本次作业中，你将在 `cs231n/layers.py` 文件中实现各层。你可以复用之前实现的 `affine_forward`、`affine_backward`、`relu_forward`、`relu_backward` 和 `softmax_loss`。目前不用担心实现 dropout 或批归一化/层归一化，这些功能稍后会添加。\n",
    "<details><summary>原文</summary>\n",
    "Read through the `FullyConnectedNet` class in the file `cs231n/classifiers/fc_net.py`.\n",
    "\n",
    "Implement the network initialization, forward pass, and backward pass. Throughout this assignment, you will be implementing layers in `cs231n/layers.py`. You can re-use your implementations for `affine_forward`, `affine_backward`, `relu_forward`, `relu_backward`, and `softmax_loss` from before. For right now, don't worry about implementing dropout or batch/layer normalization yet, as you will add those features later.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYqS6VtUu-pU",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 设置环境。\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # Set default size of plots.\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\"返回相对误差。\"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56aAEw7Ku-pU"
   },
   "outputs": [],
   "source": [
    "# 加载（预处理后的）CIFAR-10 数据。\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L60FVoXSu-pV"
   },
   "source": [
    "## 初始损失和梯度检查\n",
    "\n",
    "作为健壮性检查，运行以下内容以检查初始损失，并对网络在有无正则化的情况下进行梯度检查。这是判断初始损失是否合理的好方法。\n",
    "\n",
    "对于梯度检查，你应该期望误差在 1e-7 或更小。\n",
    "<details><summary>原文</summary>\n",
    "## Initial Loss and Gradient Check\n",
    "\n",
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. This is a good way to see if the initial losses seem reasonable.\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-7 or less.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffLH91h4u-pV"
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "    print(\"Running check with reg = \", reg)\n",
    "    model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C, reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "    loss, grads = model.loss(X, y)\n",
    "    print(\"Initial loss: \", loss)\n",
    "\n",
    "    # 大多数误差应该在 e-7 或更小的量级。\n",
    "    # 注意：当 reg = 0.0 时，W2 的误差在 e-5 量级也是可以接受的。\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "        print(f\"{name} relative error: {rel_error(grad_num, grads[name])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSiVnvNgu-pW"
   },
   "source": [
    "作为另一个健壮性检查，确保你的网络能在 50 张图片的小数据集上过拟合。首先，我们将尝试一个每个隐藏层有 100 个单元的三层网络。在下面的代码块中，通过调整**学习率**和**权重初始化尺度**来实现过拟合，并在 20 个 epoch 内达到 100% 的训练准确率。\n",
    "<details><summary>原文</summary>\n",
    "As another sanity check, make sure your network can overfit on a small dataset of 50 images. First, we will try a three-layer network with 100 units in each hidden layer. In the following cell, tweak the **learning rate** and **weight initialization scale** to overfit and achieve 100% training accuracy within 20 epochs.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYHs3IPmu-pW",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: 使用三层网络，通过调整学习率和初始化尺度，在 50 个训练样本上实现过拟合。\n",
    "# TODO: Use a three-layer Net to overfit 50 training examples by\n",
    "# tweaking just the learning rate and initialization scale.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "    \"X_train\": data[\"X_train\"][:num_train],\n",
    "    \"y_train\": data[\"y_train\"][:num_train],\n",
    "    \"X_val\": data[\"X_val\"],\n",
    "    \"y_val\": data[\"y_val\"],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-2  # Experiment with this!\n",
    "learning_rate = 1e-4  # Experiment with this!\n",
    "\n",
    "\n",
    "model = FullyConnectedNet([100, 100], weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(\n",
    "    model,\n",
    "    small_data,\n",
    "    print_every=10,\n",
    "    num_epochs=20,\n",
    "    batch_size=25,\n",
    "    update_rule=\"sgd\",\n",
    "    optim_config={\"learning_rate\": learning_rate},\n",
    ")\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history)\n",
    "plt.title(\"Training loss history\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.grid(linestyle=\"--\", linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJHKHQUIu-pX"
   },
   "source": [
    "现在，尝试使用一个每层 100 个单元的五层网络在 50 个训练样本上过拟合。同样，你需要调整学习率和权重初始化尺度，但你应该能在 20 个 epoch 内达到 100% 的训练准确率。\n",
    "<details><summary>原文</summary>\n",
    "Now, try to use a five-layer network with 100 units on each layer to overfit on 50 training examples. Again, you will have to adjust the learning rate and weight initialization scale, but you should be able to achieve 100% training accuracy within 20 epochs.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbp8Nsayu-pX"
   },
   "outputs": [],
   "source": [
    "# TODO: 使用五层网络，通过调整学习率和初始化尺度，在 50 个训练样本上实现过拟合。\n",
    "# TODO: Use a five-layer Net to overfit 50 training examples by\n",
    "# tweaking just the learning rate and initialization scale.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "    \"X_train\": data[\"X_train\"][:num_train],\n",
    "    \"y_train\": data[\"y_train\"][:num_train],\n",
    "    \"X_val\": data[\"X_val\"],\n",
    "    \"y_val\": data[\"y_val\"],\n",
    "}\n",
    "\n",
    "learning_rate = 2e-3  # Experiment with this!\n",
    "weight_scale = 1e-5  # Experiment with this!\n",
    "\n",
    "\n",
    "model = FullyConnectedNet([100, 100, 100, 100], weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(\n",
    "    model,\n",
    "    small_data,\n",
    "    print_every=10,\n",
    "    num_epochs=20,\n",
    "    batch_size=25,\n",
    "    update_rule=\"sgd\",\n",
    "    optim_config={\"learning_rate\": learning_rate},\n",
    ")\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history)\n",
    "plt.title(\"Training loss history\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.grid(linestyle=\"--\", linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOdiuVmCu-pX",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## 问题 1：\n",
    "你是否注意到训练三层网络与五层网络的难度有何不同？特别是根据你的经验，哪个网络对初始化尺度更敏感？你认为原因是什么？\n",
    "\n",
    "## 答案：\n",
    "[请在此填写]\n",
    "<details><summary>原文</summary>\n",
    "## Inline Question 1:\n",
    "Did you notice anything about the comparative difficulty of training the three-layer network vs. training the five-layer network? In particular, based on your experience, which network seemed more sensitive to the initialization scale? Why do you think that is the case?\n",
    "\n",
    "## Answer:\n",
    "[FILL THIS IN]\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWe_zEgFu-pY"
   },
   "source": [
    "# 更新规则\n",
    "到目前为止，我们一直使用普通的随机梯度下降（SGD）作为更新规则。更复杂的更新规则可以让深度网络更容易训练。我们将实现几种常用的更新规则，并与普通 SGD 进行比较。\n",
    "<details><summary>原文</summary>\n",
    "# Update rules\n",
    "So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9l25b5jAu-pY"
   },
   "source": [
    "## SGD+Momentum\n",
    "带动量的随机梯度下降（SGD+momentum）是一种广泛使用的更新规则，通常比普通 SGD 收敛更快。更多信息请参见 http://cs231n.github.io/neural-networks-3/#sgd 的 Momentum Update 部分。\n",
    "\n",
    "打开 `cs231n/optim.py` 文件，阅读顶部的文档以确保你理解 API。在 `sgd_momentum` 函数中实现 SGD+momentum 更新规则，并运行以下代码检查你的实现。你应该看到误差小于 e-8。\n",
    "<details><summary>原文</summary>\n",
    "## SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochastic gradient descent. See the Momentum Update section at http://cs231n.github.io/neural-networks-3/#sgd for more information.\n",
    "\n",
    "Open the file `cs231n/optim.py` and read the documentation at the top of the file to make sure you understand the API. Implement the SGD+momentum update rule in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than e-8.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fR5m7nuXu-pY"
   },
   "outputs": [],
   "source": [
    "from cs231n.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {\"learning_rate\": 1e-3, \"velocity\": v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "# Should see relative errors around e-8 or less\n",
    "print(\"next_w error: \", rel_error(next_w, expected_next_w))\n",
    "print(\"velocity error: \", rel_error(expected_velocity, config[\"velocity\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A5WZm78u-pY"
   },
   "source": [
    "完成后，运行以下代码，用 SGD 和 SGD+momentum 训练一个六层网络。你应该能看到 SGD+momentum 收敛更快。\n",
    "<details><summary>原文</summary>\n",
    "Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z66zjXhCu-pZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "    \"X_train\": data[\"X_train\"][:num_train],\n",
    "    \"y_train\": data[\"y_train\"][:num_train],\n",
    "    \"X_val\": data[\"X_val\"],\n",
    "    \"y_val\": data[\"y_val\"],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in [\"sgd\", \"sgd_momentum\"]:\n",
    "    print(\"Running with \", update_rule)\n",
    "    model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "    solver = Solver(\n",
    "        model,\n",
    "        small_data,\n",
    "        num_epochs=5,\n",
    "        batch_size=100,\n",
    "        update_rule=update_rule,\n",
    "        optim_config={\"learning_rate\": 5e-3},\n",
    "        verbose=True,\n",
    "    )\n",
    "    solvers[update_rule] = solver\n",
    "    solver.train()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "axes[0].set_title(\"Training loss\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[1].set_title(\"Training accuracy\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[2].set_title(\"Validation accuracy\")\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "    axes[0].plot(solver.loss_history, label=f\"loss_{update_rule}\")\n",
    "    axes[1].plot(solver.train_acc_history, label=f\"train_acc_{update_rule}\")\n",
    "    axes[2].plot(solver.val_acc_history, label=f\"val_acc_{update_rule}\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(loc=\"best\", ncol=4)\n",
    "    ax.grid(linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0EQBM5xu-pZ"
   },
   "source": [
    "## RMSProp 和 Adam\n",
    "RMSProp [1] 和 Adam [2] 是通过使用梯度二阶矩的滑动平均来为每个参数设置学习率的更新规则。\n",
    "\n",
    "在 `cs231n/optim.py` 文件中，实现 `rmsprop` 函数中的 RMSProp 更新规则，以及 `adam` 函数中的 Adam 更新规则，并使用下面的测试检查你的实现。\n",
    "\n",
    "**注意：** 请实现 Adam 的完整更新规则（包括偏差修正机制），而不是课程笔记中提到的第一个简化版本。\n",
    "\n",
    "[1] Tijmen Tieleman 和 Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma 和 Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015.\n",
    "<details><summary>原文</summary>\n",
    "## RMSProp and Adam\n",
    "RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "In the file `cs231n/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check your implementations using the tests below.\n",
    "\n",
    "**NOTE:** Please implement the _complete_ Adam update rule (with the bias correction mechanism), not the first simplified version mentioned in the course notes.\n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w_hTsl-u-pZ"
   },
   "outputs": [],
   "source": [
    "# 测试 RMSProp 实现\n",
    "from cs231n.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('cache error: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s96oB_Lmu-pa"
   },
   "outputs": [],
   "source": [
    "# 测试 Adam 实现\n",
    "from cs231n.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('v error: ', rel_error(expected_v, config['v']))\n",
    "print('m error: ', rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB7lZjH9u-pa"
   },
   "source": [
    "调试好 RMSProp 和 Adam 实现后，运行以下代码，使用这些新更新规则训练一对深度网络：\n",
    "<details><summary>原文</summary>\n",
    "Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Otf_c60Qu-pa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "    print('Running with ', update_rule)\n",
    "    model = FullyConnectedNet(\n",
    "        [100, 100, 100, 100, 100],\n",
    "        weight_scale=5e-2\n",
    "    )\n",
    "    solver = Solver(\n",
    "        model,\n",
    "        small_data,\n",
    "        num_epochs=5,\n",
    "        batch_size=100,\n",
    "        update_rule=update_rule,\n",
    "        optim_config={'learning_rate': learning_rates[update_rule]},\n",
    "        verbose=True\n",
    "    )\n",
    "    solvers[update_rule] = solver\n",
    "    solver.train()\n",
    "    print()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "axes[0].set_title('Training loss')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[1].set_title('Training accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[2].set_title('Validation accuracy')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "    axes[0].plot(solver.loss_history, label=f\"{update_rule}\")\n",
    "    axes[1].plot(solver.train_acc_history, label=f\"{update_rule}\")\n",
    "    axes[2].plot(solver.val_acc_history, label=f\"{update_rule}\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(loc='best', ncol=4)\n",
    "    ax.grid(linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTSO9l58u-pa",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## 问题 2：\n",
    "\n",
    "AdaGrad，和 Adam 一样，是一种每参数优化方法，其更新规则如下：\n",
    "\n",
    "```\n",
    "cache += dw**2\n",
    "w += - learning_rate * dw / (np.sqrt(cache) + eps)\n",
    "```\n",
    "\n",
    "John 发现当他用 AdaGrad 训练网络时，更新变得非常小，网络学习变慢。根据你对 AdaGrad 更新规则的理解，你认为为什么更新会变得很小？Adam 会有同样的问题吗？\n",
    "\n",
    "## 答案：\n",
    "[请在此填写]\n",
    "<details><summary>原文</summary>\n",
    "## Inline Question 2:\n",
    "\n",
    "AdaGrad, like Adam, is a per-parameter optimization method that uses the following update rule:\n",
    "\n",
    "```\n",
    "cache += dw**2\n",
    "w += - learning_rate * dw / (np.sqrt(cache) + eps)\n",
    "```\n",
    "\n",
    "John notices that when he was training a network with AdaGrad that the updates became very small, and that his network was learning slowly. Using your knowledge of the AdaGrad update rule, why do you think the updates would become very small? Would Adam have the same issue?\n",
    "\n",
    "## Answer:\n",
    "[FILL THIS IN]\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUktPczWu-pa"
   },
   "source": [
    "# 训练一个好模型！\n",
    "在 CIFAR-10 上训练你能得到的最好的全连接模型，并将其存储在 `best_model` 变量中。我们要求你使用全连接网络在验证集上至少达到 50% 的准确率。\n",
    "\n",
    "如果你足够仔细，准确率可以超过 55%，但本部分不要求更高分数，也不会有额外加分。下一次作业我们会让你在 CIFAR-10 上训练最好的卷积网络，我们更希望你把精力放在卷积网络上。\n",
    "\n",
    "**注意：** 在下一次作业中，你将学习如 BatchNormalization 和 Dropout 等技术，这些可以帮助你训练更强大的模型。\n",
    "<details><summary>原文</summary>\n",
    "# Train a Good Model!\n",
    "Train the best fully connected model that you can on CIFAR-10, storing your best model in the `best_model` variable. We require you to get at least 50% accuracy on the validation set using a fully connected network.\n",
    "\n",
    "If you are careful it should be possible to get accuracies above 55%, but we don't require it for this part and won't assign extra credit for doing so. Later in the next assignment, we will ask you to train the best convolutional network that you can on CIFAR-10, and we would prefer that you spend your effort working on convolutional networks rather than fully connected networks.\n",
    "\n",
    "**Note:** In the next assignment, you will learn techniques like BatchNormalization and Dropout which can help you train powerful models.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLiabOb3u-pb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: 训练你能得到的最好的 FullyConnectedNet，并将其存储在 best_model 变量中。你可以使用批归一化/层归一化和 dropout。 #\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# find batch/layer normalization and dropout useful. Store your best model in  #\n",
    "# the best_model variable.                                                     #\n",
    "################################################################################\n",
    "# *****你的代码开始（请勿删除/修改此行）*****\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "# *****你的代码结束（请勿删除/修改此行）*****\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                              代码结束                                         #\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dw4p7kIu-pb"
   },
   "source": [
    "# 测试你的模型！\n",
    "在验证集和测试集上运行你的最佳模型。你应该在验证集和测试集上都达到至少 50% 的准确率。\n",
    "<details><summary>原文</summary>\n",
    "# Test Your Model!\n",
    "Run your best model on the validation and test sets. You should achieve at least 50% accuracy on the validation set and the test set.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-mWD7-Su-pb",
    "test": "val_test_accuracy"
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs231n-25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
